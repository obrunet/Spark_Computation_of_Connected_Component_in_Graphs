{"cells":[{"cell_type":"code","execution_count":null,"id":"d3e8416d","metadata":{},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"}],"source":["%%python\n","\n","\n","import time\n","import pyspark\n","from pyspark.sql import SparkSession\n","\n","\n","BUCKET_INPUT_PATH = \"gs://iasd-input-data\"\n","DATASET_PATHS = {\n","    \"notre_dame\": f\"{BUCKET_INPUT_PATH}/web-NotreDame.txt\",\n","    \"berk_stan.txt\": f\"{BUCKET_INPUT_PATH}/web-BerkStan.txt\",\n","    \"stanford.txt\": f\"{BUCKET_INPUT_PATH}/web-Stanford.txt\",\n","    \"google.txt\": f\"{BUCKET_INPUT_PATH}/web-Google.txt\"\n","}\n","\n","dataset_path = f\"{BUCKET_INPUT_PATH}/test.txt\"\n","\n","spark_session = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark SQL basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()\n","\n","spark_context = spark_session.sparkContext\n","\n","\n","def load_rdd(path):\n","    return spark_context.textFile(path)\n","\n","\n","def preprocess_rdd(rdd):\n","    return rdd.filter(lambda x: \"#\" not in x) \\\n","                .map(lambda x: x.split(\"\\t\")) \\\n","                .map(lambda x: (int(x[0]), int(x[1])))\n","\n","rdd_raw = load_rdd(dataset_path)\n","rdd = preprocess_rdd(rdd_raw)\n","rdd.take(10)"]},{"cell_type":"code","execution_count":null,"id":"b9f20a9d","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2139a893","metadata":{},"outputs":[],"source":["%%scala\n","\n","val x = 8\n","x"]},{"cell_type":"code","execution_count":null,"id":"7f078b43","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"01a354e6","metadata":{},"source":["---"]},{"cell_type":"code","execution_count":null,"id":"0ba1e9c4","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"id":"98946f13","metadata":{},"outputs":[{"data":{"text/plain":["import org.apache.spark.sql.SparkSession\n","spark_session: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@17c957c1\n","spark_context: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4302cef1\n"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","\n","val spark_session = SparkSession\n","    .builder\n","    .appName(\"Python Spark SQL basic example\")\n","    .config(\"spark.some.config.option\", \"some-value\")\n","    .getOrCreate()\n","\n","val spark_context = spark_session.sparkContext"]},{"cell_type":"code","execution_count":4,"id":"4fae0ace","metadata":{},"outputs":[{"data":{"text/plain":["rdd_raw: org.apache.spark.rdd.RDD[String] = gs://iasd-input-data/test.txt MapPartitionsRDD[1] at textFile at <console>:27\n","res0: Array[String] = Array(# bla bla, # header, 1\t2, 2\t3, 2\t4, 4\t5)\n"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["val rdd_raw = spark_context.textFile(\"gs://iasd-input-data/test.txt\")\n","rdd_raw.take(6)"]},{"cell_type":"code","execution_count":5,"id":"38ea070a","metadata":{},"outputs":[{"data":{"text/plain":["res1: Array[String] = Array(1\t2, 2\t3, 2\t4, 4\t5, 6\t7)\n"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["rdd_raw.filter(x => !x.contains(\"#\")).take(5)"]},{"cell_type":"code","execution_count":60,"id":"6663c346","metadata":{},"outputs":[{"data":{"text/plain":["rdd: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[95] at map at <console>:36\n","res39: Array[(Long, Long)] = Array((1,2), (2,3), (2,4), (4,5), (6,7))\n"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["val rdd = rdd_raw.filter(x => !x.contains(\"#\"))\n","    .map(x => x.split(\"\\t\"))\n","    .map(x => (x(0).toLong, x(1).toLong))\n","\n","rdd.take(5)"]},{"cell_type":"code","execution_count":61,"id":"69f08152","metadata":{},"outputs":[{"data":{"text/plain":["edges: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[95] at map at <console>:36\n","res40: Array[(Long, Long)] = Array((1,2), (2,3), (2,4), (4,5), (6,7))\n"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["val edges = rdd //.map(x => (x, 0))\n","edges.take(5)"]},{"cell_type":"code","execution_count":62,"id":"fed84ede","metadata":{},"outputs":[{"data":{"text/plain":["keys: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[98] at map at <console>:38\n","values: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[101] at map at <console>:42\n","vertices: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[105] at distinct at <console>:44\n","res41: Array[Long] = Array(4, 8, 1, 5, 6, 2, 3, 7)\n"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["val keys = rdd_raw.filter(x => !x.contains(\"#\"))\n","    .map(x => x.split(\"\\t\"))\n","    .map(x => (x(1).toLong))\n","\n","val values = rdd_raw.filter(x => !x.contains(\"#\"))\n","    .map(x => x.split(\"\\t\"))\n","    .map(x => (x(0).toLong))\n","\n","val vertices = keys.union(values).distinct()\n","\n","vertices.take(10)"]},{"cell_type":"code","execution_count":58,"id":"58c08495","metadata":{},"outputs":[],"source":["//vertices.map(x => Seq(x)).take(10)"]},{"cell_type":"code","execution_count":45,"id":"c70f649d","metadata":{},"outputs":[{"data":{"text/plain":["import org.apache.spark.graphx._\n"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.graphx._"]},{"cell_type":"code","execution_count":63,"id":"355e97c7","metadata":{},"outputs":[{"ename":"<console>","evalue":"34: error: type mismatch;","output_type":"error","traceback":["<console>:34: error: type mismatch;"," found   : org.apache.spark.rdd.RDD[Long]"," required: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, ?)]","    (which expands to)  org.apache.spark.rdd.RDD[(Long, ?)]","Error occurred in an application involving default arguments.","       val graph = Graph(vertices, edges)","                         ^","<console>:34: error: type mismatch;"," found   : org.apache.spark.rdd.RDD[(Long, Long)]"," required: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[?]]","Error occurred in an application involving default arguments.","       val graph = Graph(vertices, edges)","                                   ^",""]}],"source":["val graph = Graph(vertices, edges)"]},{"cell_type":"code","execution_count":null,"id":"b57573bb","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"}},"nbformat":4,"nbformat_minor":5}