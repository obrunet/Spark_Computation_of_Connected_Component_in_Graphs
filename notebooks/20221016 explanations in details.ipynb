{"cells":[{"cell_type":"code","execution_count":1,"id":"a934fbf3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","__________ nb of clusters' nodes: 1 - dataset: test_test - method: rdd __________\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #1:\t8\n","Number of new pairs for iteration #2:\t30\n","Number of new pairs for iteration #3:\t47\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #4:\t51\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #5:\t51\n","\n","No new pair, end of computation\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Nb of connected components in the graph: 2\n","Duration in seconds: 26.928279638290405\n","\n","\n","\n","__________ nb of clusters' nodes: 1 - dataset: test_test - method: df __________\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #1:\t55\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #2:\t64\n","Number of new pairs for iteration #3:\t68\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Number of new pairs for iteration #4:\t68\n","\n","No new pair, end of computation\n","Nb of connected components in the graph: 2\n","Duration in seconds: 18.64290738105774\n"]}],"source":["import time\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import IntegerType\n","\n","\n","BUCKET_INPUT_PATH = \"gs://iasd-input-data\"\n","BUCKET_OUTPUT_PATH = \"gs://iasd-output\"\n","NB_WORKER_NODES = 1  # ------- to be changed at each run / used in results ------- #\n","\n","\n","# create a spark session and retrieve the spark context from it\n","spark_session = SparkSession \\\n","    .builder \\\n","    .appName(\"PySpark App by Olivier & Jean-Loulou\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()\n","spark_context = spark_session.sparkContext\n","\n","# initialize nb_new_pair as a spark accumulator\n","nb_new_pair = sc.accumulator(0)\n","\n","\n","def load_rdd(path):\n","    \"\"\"Recieve the path of file to load and return an RDD of the dataset\"\"\"\n","    return spark_context.textFile(path)\n","\n","\n","def load_df(path):\n","    \"\"\"Recieve the path of file to load and return a DF of the dataset\"\"\"\n","    return spark_session.read.format(\"csv\").option(\"header\",\"false\")\\\n","                .load(path)\n","\n","\n","def preprocess_rdd(rdd):\n","    \"\"\"Recieve an RDD with the raw data and return a new RDD without multilines headers\n","    starting with '#', and columns splitted according to the tab separator\"\"\"\n","    return rdd.filter(lambda x: \"#\" not in x) \\\n","                .map(lambda x: x.split(\"\\t\")) \\\n","                .map(lambda x: (int(x[0]), int(x[1])))\n","\n","\n","def preprocess_df(df):\n","    \"\"\"Recieve a DF with the raw data and return a new DF without multilines headers\n","    starting with '#', and columns splitted according to the tab separator\"\"\"\n","    col_name = df.columns[0]\n","    return df.filter(f\"{col_name} NOT LIKE '#%'\")\\\n","                .withColumn('k', split(df[col_name], '\\t').getItem(0)) \\\n","                .withColumn('v', split(df[col_name], '\\t').getItem(1)) \\\n","                .drop(col_name)\\\n","                .withColumn(\"k\",col(\"k\").cast(IntegerType())) \\\n","                .withColumn(\"v\",col(\"v\").cast(IntegerType()))\n","\n","\n","def iterate_map_rdd(rdd):\n","    \"\"\"Recieve an RDD with (k, v) and return a new RDD resulting of the concatenation of the \n","    original input RDD and itself where keys & values were inverted (v, k)\"\"\"\n","    return rdd.union(rdd.map(lambda x : (x[1], x[0])))\n","\n","\n","def iterate_map_df(df):\n","    \"\"\"Recieve a DF with 2 columns 'k', 'v' and return a new DF resulting of the concatenation \n","    of the original input DF and itself where columns were inverted 'k', 'v'\"\"\"\n","    return df.union(df.select(col(\"v\").alias(\"k\"), col(\"k\").alias(\"v\")))\n","\n","\n","# countnb_new_pair function to know if additional CCF iteration is needed\n","def count_nb_new_pair(x):\n","    \"\"\"Count the number of new pairs - function used in iterate_reduce_rdd in order to \n","    determine if new edges were attached in a component and if the process is over or not\"\"\"\n","    global nb_new_pair\n","    k, values = x\n","    min, value_list = k, []\n","    for v in values:\n","        if v < min:\n","            min = v\n","        value_list.append(v)\n","    if min < k:\n","        yield((k, min))\n","        for v in value_list:\n","            if min != v:\n","                nb_new_pair += 1\n","                yield((v, min))\n","        \n","\n","def iterate_reduce_rdd(rdd):\n","    \"\"\"Recieve an RDD alreday processed by iterate_map_rdd(), and for each component\n","    count new pairs with count_nb_new_pair(), the return RDD is sorted by keys\"\"\"\n","    return rdd.groupByKey().flatMap(lambda x: count_nb_new_pair(x)).sortByKey()\n","\n","\n","def iterate_reduce_df(df):\n","    \"\"\"Recieve a DF alreday processed by iterate_map_df(), and for each component\n","    count new pairs, the return DFrrrrrrrrrrrrrrrrrrrrrrrr\"\"\"    \n","    global nb_new_pair\n","    df = df.groupBy(col(\"k\")).agg(collect_set(\"v\").alias(\"v\"))\\\n","                                            .withColumn(\"min\", least(col(\"k\"), array_min(\"v\")))\\\n","                                            .filter((col(\"k\")!=col('min')))\n","\n","    nb_new_pair += df.withColumn(\"count\", size(\"v\")-1).select(sum(\"count\")).collect()[0][0]\n","\n","    return df.select(col(\"min\").alias(\"a_min\"), concat(array(col(\"k\")), col(\"v\")).alias(\"valueList\"))\\\n","                                                    .withColumn(\"valueList\", explode(\"valueList\"))\\\n","                                                    .filter((col('a_min')!=col('valueList')))\\\n","                                                    .select(col('a_min').alias(\"k\"), col('valueList').alias(\"v\"))\n","\n","\n","def compute_cc_rdd(rdd):\n","    \"\"\"Recieve a preprocessed RDD and compute CCF according to several iterations jobs of iterate_map/reduce_rdd, dedup.\n","    When no new pair were counted: return an RDD with the componentIDs of each group of connected edges\"\"\"\n","    nb_iteration = 0\n","    while True:\n","        nb_iteration += 1\n","        start_pair = nb_new_pair.value\n","\n","        rdd = iterate_map_rdd(rdd)\n","        rdd = iterate_reduce_rdd(rdd)\n","        rdd = rdd.distinct()  # used for iterate_dedup / deduplication\n","\n","        print(f\"Number of new pairs for iteration #{nb_iteration}:\\t{nb_new_pair.value}\")\n","        if start_pair == nb_new_pair.value:\n","            print(\"\\nNo new pair, end of computation\")\n","            break\n","    return rdd\n","\n","\n","def compute_cc_df(df):\n","    \"\"\"Recieve a preprocessed DF and compute CCF according to several iterations jobs of iterate_map/reduce_df, dedup.\n","    When no new pair were counted: return a DF with the componentIDs of each group of connected edges\"\"\"\n","    nb_iteration = 0\n","    while True:\n","        nb_iteration += 1\n","        nb_pairs_start = nb_new_pair.value\n","\n","        df = iterate_map_df(df)\n","        df = iterate_reduce_df(df)\n","        df = df.distinct()\n","        \n","        print(f\"Number of new pairs for iteration #{nb_iteration}:\\t{nb_new_pair.value}\")\n","        if nb_pairs_start == nb_new_pair.value:\n","            print(\"\\nNo new pair, end of computation\")\n","            break\n","    return df\n","\n","\n","def workflow_rdd(path):\n","    \"\"\"Recieve the dataset path, and realize all the transformations / computation in RDD: loading,\n","    preprocessing, CCF computation, calculate time and the number of distinct connected components\"\"\"\n","    rdd_raw = load_rdd(path)\n","    rdd = preprocess_rdd(rdd_raw)\n","    start_time = time.time()\n","    rdd = compute_cc_rdd(rdd)\n","    print(f\"Nb of connected components in the graph: {rdd.map(lambda x : x[1]).distinct().count()}\")\n","    print(f\"Duration in seconds: {time.time() - start_time}\")\n","\n","\n","def workflow_df(path):\n","    \"\"\"Recieve the dataset path, and realize all the transformations / computation in DF: loading,\n","    preprocessing, CCF computation, calculate time and the number of distinct connected components\"\"\"\n","    df_raw = load_df(path)\n","    df = preprocess_df(df_raw)\n","    start_time = time.time()\n","    df = compute_cc_df(df)\n","    print(f\"Nb of connected components in the graph: {df.select('k').distinct().count()}\")\n","    print(f\"Duration in seconds: {time.time() - start_time}\")   \n","    \n","\n","def main():\n","    \n","    dataset_paths = {\n","        \"test_test\": f\"{BUCKET_INPUT_PATH}/test.txt\"\n","#         \"notre_dame\": f\"{BUCKET_INPUT_PATH}/web-NotreDame.txt\",\n","#         \"berk_stan\": f\"{BUCKET_INPUT_PATH}/web-BerkStan.txt\",\n","#         \"stanford\": f\"{BUCKET_INPUT_PATH}/web-Stanford.txt\",\n","#         \"google\": f\"{BUCKET_INPUT_PATH}/web-Google.txt\"\n","    }\n","    computation_methods = {\n","        \"rdd\": workflow_rdd,\n","        \"df\": workflow_df\n","    }\n","    \n","    # loop on all the datasets & methods (RDD or DF) using previsous dictionnaries\n","    for dataset in dataset_paths.keys():\n","        for method in computation_methods.keys():\n","            print(\"\\n\"* 3 + \"_\" * 10 + \n","                  f\" nb of clusters' nodes: {NB_WORKER_NODES} - dataset: {dataset} - method: {method} \"\n","                  + \"_\" * 10)\n","            computation_methods[method](dataset_paths[dataset])\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"id":"c962379d","metadata":{},"outputs":[],"source":["    rdd_raw = load_rdd(path)\n","    rdd = preprocess_rdd(rdd_raw)\n","    start_time = time.time()\n","    rdd = compute_cc_rdd(rdd)"]},{"cell_type":"code","execution_count":null,"id":"96854da2","metadata":{},"outputs":[],"source":["    df_raw = load_df(path)\n","    df = preprocess_df(df_raw)\n","    start_time = time.time()\n","    df = compute_cc_df(df)"]},{"cell_type":"code","execution_count":null,"id":"92773d21","metadata":{},"outputs":[],"source":["        df = iterate_map_df(df)\n","        df = iterate_reduce_df(df)\n","        df = df.distinct()"]},{"cell_type":"code","execution_count":3,"id":"84455a77","metadata":{},"outputs":[],"source":["path = f\"{BUCKET_INPUT_PATH}/test.txt\""]},{"cell_type":"markdown","id":"d9289a0d","metadata":{},"source":["file loading"]},{"cell_type":"code","execution_count":4,"id":"f1e110ce","metadata":{},"outputs":[{"data":{"text/plain":["['# bla bla', '# header', '1\\t2', '2\\t3', '2\\t4', '4\\t5']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["rdd_raw = load_rdd(path)\n","rdd_raw.take(6)"]},{"cell_type":"code","execution_count":5,"id":"b66cde69","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|      _c0|\n","+---------+\n","|# bla bla|\n","| # header|\n","|      1\t2|\n","|      2\t3|\n","|      2\t4|\n","|      4\t5|\n","+---------+\n","only showing top 6 rows\n","\n"]}],"source":["df_raw = load_df(path)\n","df_raw.show(6)"]},{"cell_type":"markdown","id":"11399c66","metadata":{},"source":["dataset preprocessing"]},{"cell_type":"code","execution_count":7,"id":"69eff327","metadata":{},"outputs":[{"data":{"text/plain":["[(1, 2), (2, 3), (2, 4), (4, 5), (6, 7), (7, 8)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["rdd = preprocess_rdd(rdd_raw)\n","rdd.take(10)"]},{"cell_type":"code","execution_count":8,"id":"77d338e8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---+\n","|  k|  v|\n","+---+---+\n","|  1|  2|\n","|  2|  3|\n","|  2|  4|\n","|  4|  5|\n","|  6|  7|\n","|  7|  8|\n","+---+---+\n","\n"]}],"source":["df = preprocess_df(df_raw)\n","df.show(10)"]},{"cell_type":"markdown","id":"a286ecd0","metadata":{},"source":["iterate map"]},{"cell_type":"code","execution_count":9,"id":"9f04bcdd","metadata":{},"outputs":[{"data":{"text/plain":["[(1, 2),\n"," (2, 3),\n"," (2, 4),\n"," (4, 5),\n"," (6, 7),\n"," (7, 8),\n"," (2, 1),\n"," (3, 2),\n"," (4, 2),\n"," (5, 4),\n"," (7, 6),\n"," (8, 7)]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["rdd = iterate_map_rdd(rdd)\n","rdd.take(20)"]},{"cell_type":"code","execution_count":10,"id":"59e9d800","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---+\n","|  k|  v|\n","+---+---+\n","|  1|  2|\n","|  2|  3|\n","|  2|  4|\n","|  4|  5|\n","|  6|  7|\n","|  7|  8|\n","|  2|  1|\n","|  3|  2|\n","|  4|  2|\n","|  5|  4|\n","|  7|  6|\n","|  8|  7|\n","+---+---+\n","\n"]}],"source":["df = iterate_map_df(df)\n","df.show(20)"]},{"cell_type":"markdown","id":"49d0b8f1","metadata":{},"source":["iterate reduce"]},{"cell_type":"code","execution_count":12,"id":"684c5725","metadata":{},"outputs":[{"data":{"text/plain":["[(2, 1),\n"," (3, 1),\n"," (3, 2),\n"," (4, 2),\n"," (4, 1),\n"," (5, 2),\n"," (5, 4),\n"," (7, 6),\n"," (8, 7),\n"," (8, 6)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["rdd = iterate_reduce_rdd(rdd)\n","rdd.take(16)"]},{"cell_type":"code","execution_count":11,"id":"0340b112","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---+\n","|  k|  v|\n","+---+---+\n","|  2|  3|\n","|  4|  5|\n","|  2|  4|\n","|  2|  5|\n","|  6|  7|\n","|  6|  8|\n","|  7|  8|\n","|  1|  2|\n","|  1|  3|\n","|  1|  4|\n","+---+---+\n","\n"]}],"source":["df = iterate_reduce_df(df)\n","df.show()"]},{"cell_type":"code","execution_count":null,"id":"db77d796","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}